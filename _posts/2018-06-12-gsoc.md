---
layout:     post
title:      An introduction to Tensorflex
date:       2018-06-12 00:00:00
summary:    A tutorial explaining how Tensorflex can be used to generate predictions on saved Tensorflow graph models  
categories: gsoc
---

## The aim of this blog post

This blog post serves to function as a tutorial for using the project I have been working on as part of Google Summer of Code, called Tensorflex. Tensorflex is a bridge to Tensorflow, Google's machine learning framework. Follow the work on Github [here](https://github.com/anshuman23/tensorflex) and star the repository if it helps you out!

This post will first touch upon using Python for training and saving models and then on how to make predicitions from the saved model using Tensorflex. 

## Tensorflow and Tensorflex
- Tensorflow is Google's machine learning framework and is the de-facto standard for training models and making predictions in industry and academia (along with PyTorch). 
- The best way to use Tensorflow for training models is through Python. The C++ API can also be used but is generally more cumbersome and low-level than needed. Moreover, documentation for the Tensorflow Python API is the most easy to read and understand.
- Tensorflex consist of the Tensorflow bindings for Elixir. It is based on the Tensorflow C API which only supports making predictions from saved models at the moment.
- Tensorflex can be used for making predictions in Elixir from trained models written in Python.
- The first half of this post covers using Python for training a simple neural network model on the widely known [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)
- The second half covers making predictions from a set of inputs using Tensorflex

## Introduction to computational graphs

<div style="text-align:center" markdown="1">
![alt text](https://raw.githubusercontent.com/anshuman23/blog/master/images/cgraph.png)
</div>

Computational graphs form the basis for how Tensorflow works. The figure shown above is an example of a computational graph. As is evident, a graph consists of a set of operations that are performed on the inputs provided to the graph. Tensorflow uses this graph to:
- describe the architecture for your machine learning model
- save this graph (after training) so that predictions can be made by performing the same set of actions (read "operations) for the input provided at the moment

Therefore, without too much further ado, we now discuss the dataset to be used for our ML model, the Python Tensorflow code and then Tensorflex.

## The Iris dataset
This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. The predicted attribute is the class of iris plant. 

- Attribute Information:
  - sepal length in cm 
  - sepal width in cm 
  - petal length in cm 
  - petal width in cm 
  - class: 
    - Iris Setosa 
    - Iris Versicolour 
    - Iris Virginica
    
The model can be downloaded as a CSV file [here](https://github.com/anshuman23/tensorflex/blob/master/examples/iris-example/dataset.csv) 

## Python code for training a simple model on the Iris dataset

This code has been adopted from the original Github repository [here](https://github.com/EdoVaira/Iris-Neural-Network). The entire code that is to follow is available as an example in the Tensorflex repository. Here is the [link](https://github.com/anshuman23/tensorflex/tree/master/examples/iris-example). 

The first step requires cloning the Tensorflex repository, and then you can navigate to the `examples/iris-example` folder and run the example directly from there using `python train_model.py` on the command line.


A brief description of what is happening in the `train_model.py` Python code:


We first make all the required imports, and then read in the entire dataset from the `CSV` file. We then put the features in the `X` NumPy array and assign the class labels to the `y` NumPy vector. Then in the last few lines of this code snippet we shuffle the data using the `np.random.choice` function and assign these shuffled values from `X` to `X_values` and `y` to `y_values` respectively. Also, it is important to note, when we read in the class labels, we encode them as one-hot variables because the neural network can only understand numeral values and not strings.

{% highlight python %}
import tensorflow as tf
import pandas as pd
import numpy as np
from tensorflow.python.tools import freeze_graph
import os

dir = os.path.dirname(os.path.realpath(__file__)) + '/data/'

seed = 1234
np.random.seed(seed)
tf.set_random_seed(seed)

dataset = pd.read_csv('dataset.csv')
dataset = pd.get_dummies(dataset, columns=['Species']) 
values = list(dataset.columns.values)

y = dataset[values[-3:]]
y = np.array(y, dtype='float32')
X = dataset[values[1:-3]]
X = np.array(X, dtype='float32')

indices = np.random.choice(len(X), len(X), replace=False)
X_values = X[indices]
y_values = y[indices]
{% endhighlight %}

Next, we separate the entire data into train and test sets. Since, we want to feed in the input later on into Elixir manually and see the results, we set the test dataset size to just 10 values. We also declare our session variable to be used for computation later on.

Then we define the neural network architecture for our model. As can be seen it consists of the following operations:
- A `placeholder` for our input data/features, which can consist of any number of rows and 4 columns (corresponding to the 4 features)
- A `placeholder` for the class labels corresponding to the input features which has the same number of rows as the input, but 3 columns (For the 3 classes, after one-hot encoding)
- There are two hidden layer and both have 8 nodes
- The first set of weights to be multipled to the input, the biases to be added to this product
- The second set of weights to be multiplied to the matrix sum obtained in the previous step and the second set of biases to be added to this product
- The obtained matrix sum is passed through to the softmax function to obtain the output. Softmax squashes the results into values between 0 and 1 which resemble probabilities (although they are mathematically not). Thus one can see which class was predicted by observing which class has the highest numerical value at the output

__It is extremely important to understand that operations need to be named. This is crucial to reusing this graph later for making predictions in Elixir. Moreover, it is important to name the inputs as well as the outputs, as these are going to be referred to when feeding input data and obtaining outputs in Elixir. In our case, we name the input operation `input` and the output operation `output`.__

{% highlight python %}
test_size = 10
X_test = X_values[-test_size:]
X_train = X_values[:-test_size]
y_test = y_values[-test_size:]
y_train = y_values[:-test_size]

sess = tf.Session()

interval = 50
epoch = 500

X_data = tf.placeholder(shape=[None, 4], dtype=tf.float32, name='input')
y_target = tf.placeholder(shape=[None, 3], dtype=tf.float32)

hidden_layer_nodes = 8

w1 = tf.Variable(tf.random_normal(shape=[4,hidden_layer_nodes]), name='weights1') 
b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]), name='biases1')   
w2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes,3]), name='weights2') 
b2 = tf.Variable(tf.random_normal(shape=[3]),name='biases2')


hidden_output = tf.nn.relu(tf.add(tf.matmul(X_data, w1), b1))
final_output = tf.nn.softmax(tf.add(tf.matmul(hidden_output, w2), b2), name='output')
{% endhighlight %}

{% highlight python %}

{% endhighlight %}

## Generating predictions in Elixir using Tensorflex

## Conclusion

